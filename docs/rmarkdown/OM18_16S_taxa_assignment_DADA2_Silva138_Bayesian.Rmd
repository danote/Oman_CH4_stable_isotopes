---
---
---
title: "16S rRNA gene processing and taxonomic assignment on DNA extracted from samples obtained in Oman in 2018"
subtitle: "Source file: OM18_16S_taxa_assignment_DADA2_Silva138_Bayesian.Rmd"
author: "Daniel Nothaft"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output:
  html_document: 
    df_print: paged # omit to disable paged data table output
    css: stylesheet.css # omit if no need for custom stylesheet
    number_sections: yes # change to no for unnumbered sections
    toc: yes # change to no to disable table of contents
    toc_float: true # change to false to keep toc at the top
    toc_depth: 3 # change to specify which headings to include in toc
    code_folding: show # change to hide to hide code by default
editor_options:
  chunk_output_type: inline
---

# DADA2 Setup

Data processed using the packages [dada2](https://benjjneb.github.io/dada2/index.html) version `r packageVersion("dada2")`.

Following [this tutorial](https://benjjneb.github.io/dada2/tutorial.html)

Load packages
```{r setup}
library(dada2)#; packageVersion("dada2") # processing sequence data
library(tidyverse) # manipulating strings and data frames

# global knitting options for automatic saving of all plots as .png and .pdf. Also sets cache directory.
knitr::opts_chunk$set(
  dev = c("png", "pdf"), fig.keep = "all",
  dev.args = list(pdf = list(encoding = "WinAnsi", useDingbats = FALSE)),
  fig.path = file.path("fig_output/", paste0(gsub("\\.[Rr]md", "/", knitr::current_input()))),
  cache.path = file.path("cache/", paste0(gsub("\\.[Rr]md", "/", knitr::current_input())))
)
```

Define where the fastq file are and check what the files are. These fastq files were previously demultiplexed with QIIME2.
```{r set-fastq-files-directory}
path <- "~/Desktop/Boulder/2018_fall/molecular_methods_ebio/Molecular-Methods/dada2/attempt_1_files/demux_extracted_OM18/data_for_Oman_CH4_stable_isotope_paper/fastq_files" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```

Now we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.
```{r read-fastq-files}
# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
```

# Extract sample names
```{r extract-sample-names}
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_L001"), `[`, 1)

sample.names
```

My fastq files had extra digits on the back end of the sample names, so this next chunk trims those from the sample names.
```{r trim-sample-names}
sample.names.rev <- sapply(lapply(strsplit(sample.names, NULL), rev), paste, collapse="") # reverse sample name strings because many stringr function like to work from start of string

sample.names.rev.nums.cut <- sub("^[^_]*", "", sample.names.rev) # cut the strings up til first underscore

sample.names.rev.nums.cut <- str_remove(sample.names.rev.nums.cut, "_") # remove underscore

sample.names <- sapply(lapply(strsplit(sample.names.rev.nums.cut, NULL), rev), paste, collapse="") # reverse cut strings back into original order

sample.names # print
```

# Inspect read quality profiles
We start by visualizing the quality profiles of the forward reads:
```{r plot-quality-profile-forward}
plotQualityProfile(fnFs[1:4])
```

In gray-scale is a heat map of the frequency of each quality score at each base position. The difference between a quality score of 40 and a quality score of 20 is an expected error rate of 1 in 10,000 vs 1 in 100 ([Phred](https://en.wikipedia.org/wiki/Phred_quality_score)). The median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same lenghth, hence the flat red line).

The forward reads are good quality. DADA2 generally advises trimming the last few nucleotides to avoid less well-controlled errors that can arise there. These quality profiles do not suggest that any additional trimming is needed. We will truncate the forward reads at position 145 (trimming the last 15 nucleotides).

Now we visualize the quality profile of the reverse reads:

```{r plot-quality-profile-reverse}
plotQualityProfile(fnRs[1:4])
```

The reverse reads also look to be of good quality.

# Filter reads
Make folder for filtered reads and rename files
```{r create-directory-for-filtered-files}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
```

Filter arguments:

1. truncLen sets the length at which reads should be truncated which is also the minimum length of a read required to pass the filter for the (forward,reverse) reads, default is 0 and no reads are truncated.
2. maxN sets the maximum number of N reads allowed in a sequence, by default dada2 doesn't allow any Ns
3. maxEE sets the maximum number of expected errors that are accepted before a read is tossed. default is infinity.
4. truncQ truncates the read once a quality score at or below truncQ is hit.
5. rm.phix discards reads that match the phiX genome, default is TRUE
6. matchIDs default is FALSE which assumes that paired end reads are in matching order, TRUE only outputs reads where both forward and reverse can be found and matched.
7. compress default is TRUE if fastq files are gzipped
8. multithread allows for parallelization, default is FALSE

We’ll use standard filtering parameters: maxN=0 (DADA2 requires no Ns), truncQ=2, rm.phix=TRUE and maxEE=2. The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores. (https://academic.oup.com/bioinformatics/article/31/21/3476/194979)

```{r filter-and-trim}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(145,145),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```

# Learn errors

The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The  learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r learn-errors-forward}
errF <- learnErrors(filtFs, multithread=TRUE)
```

Reverse:
```{r learn-errors-reverse}
errR <- learnErrors(filtRs, multithread=TRUE)
```

Visualize estimated error rates.
```{r visualize-estimated-error-rates}
plotErrors(errF, nominalQ=TRUE)
```

The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence.

## Dereplication
Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance” equal to the number of reads with that unique sequence. Dereplication substantially reduces computation time by eliminating redundant comparisons.

Dereplication in the DADA2 pipeline has one crucial addition from other pipelines: DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent sample inference step, significantly increasing DADA2’s accuracy.
```{r dereplicate, message=FALSE}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

# Sample inference
We are now ready to apply [the core sample inference algorithm](https://www.nature.com/articles/nmeth.3869#methods)  to the dereplicated data.
```{r infer-samples-forward}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
```

```{r infer-samples-reverse}
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
```

Inspecting the returned dada-class object:
```{r inspect-returned-dada-object-forward}
dadaFs[[1]]
```
The DADA2 algorithm inferred 103 true sequence variants from the 5305 unique sequences in the first sample.

# Merging paired ends
We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region.
```{r merge-paired-ends}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```
The mergers object is a list of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.

# Construct sequence table
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.
```{r construct-sequence-table}
# create sequence table
seqtab <- makeSequenceTable(mergers)
# show dimenions of sequence table
dim(seqtab)
```

```{r inspect-seq-lengths}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
This table shows the number of sequences in the dataset of various lengths. Here, you want to check that the lengths of our merged sequences all fall within the expected range for your amplicon. The sequences in this dataset range from 204-278 bp length, which seems reasonable.

# Remove chimeras
The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of the sequence variants after denoising makes identifying chimeras simpler than it is when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r remove-chimeras}
# remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
# show dimensions of resulting dataframe
dim(seqtab.nochim)
```

This double checks that you didn't just toss most of your reads, if this percentage is low it means that few of your total reads remain and then you need to go back. In this dataset, chimeras account for only about 3% of the merged sequence reads.
```{r check-chimera-proportion}
sum(seqtab.nochim)/sum(seqtab)
```

# Check pipeline filtering by step
Hopefully you kept most of the input reads, and there shouldn't be one step where most of the reads were lost.
```{r check-pipeline-filtering}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
Looks good! We kept the majority of our raw reads, and there is no over-large drop associated with any single step.

# Assign taxonomy
It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to assign taxonomy to the sequence variants. The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The assignTaxonomy function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least minBoot bootstrap confidence.

Here, we use the Silva 138 training set.
```{r assign-taxonomy}
taxa_Silva <- assignTaxonomy(seqtab.nochim, "~/Desktop/Boulder/2018_fall/molecular_methods_ebio/Molecular-Methods/dada2/silva_nr_v138_train_set.fa.gz", multithread=TRUE)
```

Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments.

```{r add-species}
taxa_Silva <- addSpecies(taxa_Silva, "/Users/melo.d/Desktop/Boulder/2018_fall/molecular_methods_ebio/Molecular-Methods/dada2/silva_species_assignment_v138.fa.gz")
```

Let’s inspect the taxonomic assignments:
```{r inspect-taxa-assignments}
taxa.print <- taxa_Silva # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```


# Create output tables
(courtesy of Mike Lee aka Astrobio Mike)
```{r create-output-tables}
 # giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

  # making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs_Silva_Bayes.fa")

  # count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts_Silva_Bayes.txt", sep="\t", quote=F)

  # tax table:
asv_tax <- taxa_Silva
row.names(asv_tax) <- sub(">", "", asv_headers)
write.table(asv_tax, "ASVs_taxonomy_Silva_Bayes.txt", sep="\t", quote=F)
```

DBN making these taxa tables in format mctoolsr will accept
```{r format-output-tables-for-mctoolsr}
asv_tax_one_string <- as.data.frame(asv_tax) # make taxa table a dataframe

# make a single string out of all the taxa levels
asv_tax_one_string$taxonomy <- paste0("k__", asv_tax_one_string$Kingdom, "; p__", asv_tax_one_string$Phylum, "; c__", asv_tax_one_string$Class, "; o__", asv_tax_one_string$Order, "; f__", asv_tax_one_string$Family, "; g__", asv_tax_one_string$Genus, "; s__", asv_tax_one_string$Species)

asv_IDs_tax <- as.data.frame(rownames(asv_tax_one_string)) # make a dataframe of ASV ID's
colnames(asv_IDs_tax) <- c("#OTU ID") # Add ASV ID's to taxa table as a row

asv_tax_one_string <- bind_cols(asv_IDs_tax,  asv_tax_one_string) # adding together data frames containing ASV ID's and the single string taxonomic classification

asv_tax_one_string <- asv_tax_one_string %>% select(-Kingdom, -Phylum, -Class, -Order, -Family, -Genus, -Species) # remove rows containing taxonomic classification by individual taxonomic levels

asv_tab_df <- as.data.frame(asv_tab) # convert table of ASV counts to dataframe

asv_IDs_asv_tab <- as.data.frame(rownames(asv_tab_df)) # make dataframe of ASV ID's
colnames(asv_IDs_asv_tab) <- ("#OTU ID") # Name column containing ASV ID's

asv_tab_df <- bind_cols(asv_IDs_asv_tab,  asv_tab_df) # Add together dataframes of ASV ID's and ASV counts

tax_tab_for_mctoolsr_Silva <- asv_tab_df %>% right_join(asv_tax_one_string, by = '#OTU ID') # join data frames of taxonomic classifications and counts

head(tax_tab_for_mctoolsr_Silva) # print first few rows
```

# Export output table
```{r export-output-table}
write.table(tax_tab_for_mctoolsr_Silva, "data_output/tax_tab_DADA2_mctoolsr_OM18_Silva132_Bayes.txt", row.names = FALSE, sep="\t", quote=F) # write a .txt file of the taxa table
```